[{"content":"Probability Identities 1. Conditional Bayes’ Formula Let $A, B, C$ be events. Then\n$$ \\frac{P(A \\mid B, C)}{1} = \\frac{P(C \\mid A, B) , P(A \\mid B)}{P(C \\mid B)}. $$\nApplication.\nIn estimating whether a coin is biased, suppose the first toss is $H$ and the second toss is also $H$:\n$A =$ “the coin is biased” $B =$ “first toss is $H$” $C =$ “second toss is $H$” Then Bayes’ rule lets you update the posterior after observing the second toss without recomputing from scratch.\nLinear Algebra Identities 1. Sylvester’s Criterion A matrix is positive semidefinite if and only if all principal minors have nonnegative determinant.\n2. Diagonal Dominance A matrix that is symmetric and diagonally dominant is positive definite.\n3. Lévy–Desplanques Theorem See:\nhttps://planetmath.org/levydesplanquestheorem\n4. Covariance vs.\\ Correlation Matrix Let $C$ be the covariance matrix and $\\tilde{C}$ the correlation matrix.\nLet $D$ be diagonal with entries equal to marginal standard deviations. Then\n$$ C = D \\tilde{C} D. $$\nArithmetic Identities 1. Geometric Series 1.1 Infinite geometric series ($|p|\u0026lt;1$) $$ \\sum_{k=0}^{\\infty} p^k = \\frac{1}{1 - p}. $$\nProof. Let $S = \\sum_{k=0}^{\\infty} p^k$. Then\n$pS = p + p^2 + p^3 + \\cdots$ $S = 1 + p + p^2 + p^3 + \\cdots$ Subtract:\n$$ (1 - p) S = 1 ;\\Rightarrow; S = \\frac{1}{1 - p}. $$\n1.2 Finite geometric series $$ \\sum_{k=0}^{n - 1} p^k = \\frac{1 - p^n}{1 - p}. $$\n2. Geometric Series II (Derivative Trick) For $|p|\u0026lt;1$,\n$$ \\frac{1}{(1 - p)^2} = \\sum_{k=1}^{\\infty} k p^{k - 1}. $$\n3. Summation I $$ \\sum_{k=1}^{n} k = \\frac{n(n + 1)}{2}. $$\n4. Summation II $$ \\sum_{k=1}^{n} k^2 = \\frac{n(n+1)(2n+1)}{6}. $$\nProof (Telescoping) Consider\n$$ \\sum_{k=1}^{n} \\big[ (k+1)^3 - k^3 \\big]. $$\nThe left side equals $(n+1)^3 - 1$.\nThe right side:\n$$ (k+1)^3 - k^3 = 3k^2 + 3k + 1. $$\nSo\n$$ (n+1)^3 - 1 = 3 \\sum_{k=1}^n k^2 + 3 \\sum_{k=1}^n k + n. $$\nSubstitute $\\sum k = \\frac{n(n+1)}{2}$ and solve for $\\sum k^2$:\n$$ \\sum_{k=1}^n k^2 = \\frac{1}{3} n^3 + \\frac{1}{2} n^2 + \\frac{1}{6} n. $$\n5. Cauchy–Schwarz Inequality (a) Summation Form $$ \\left( \\sum_{i=1}^n a_i b_i \\right)^2 \\le \\left( \\sum_{i=1}^n a_i^2 \\right) \\left( \\sum_{i=1}^n b_i^2 \\right). $$\n(b) Integral Form $$ \\left( \\int fg \\right)^2 \\le \\left( \\int f^2 \\right) \\left( \\int g^2 \\right). $$\n(c) Expectation Form $$ \\mathbb{E}[XY]^2 \\le \\mathbb{E}[X^2] , \\mathbb{E}[Y^2]. $$\n(d) Covariance Form $$ \\mathrm{Cov}(X,Y)^2 \\le \\mathrm{Var}(X) , \\mathrm{Var}(Y). $$\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-19-important-identities/","summary":"\u003ch1 id=\"probability-identities\"\u003eProbability Identities\u003c/h1\u003e\n\u003ch2 id=\"1-conditional-bayes-formula\"\u003e1. Conditional Bayes’ Formula\u003c/h2\u003e\n\u003cp\u003eLet $A, B, C$ be events. Then\u003c/p\u003e\n\u003cp\u003e$$\n\\frac{P(A \\mid B, C)}{1}\n=\n\\frac{P(C \\mid A, B) , P(A \\mid B)}{P(C \\mid B)}.\n$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eApplication.\u003c/strong\u003e\u003cbr\u003e\nIn estimating whether a coin is biased, suppose the first toss is $H$ and the second toss is also $H$:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$A =$ “the coin is biased”\u003c/li\u003e\n\u003cli\u003e$B =$ “first toss is $H$”\u003c/li\u003e\n\u003cli\u003e$C =$ “second toss is $H$”\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen Bayes’ rule lets you update the posterior after observing the second toss \u003cem\u003ewithout recomputing from scratch\u003c/em\u003e.\u003c/p\u003e","title":"Useful identities"},{"content":" Question Suppose $X \\sim \\mathcal{N}(\\mu_x, \\sigma_x^2)$ and $Y \\sim \\mathcal{N}(\\mu_y, \\sigma_y^2)$ are jointly Gaussian with correlation $\\rho$. Compute $$ \\mathbb{E}[e^Y \\mid X]. $$ Solution The key step is to identify the conditional distribution of $Y$ given $X$.\nFor jointly Gaussian $(X,Y)$, the conditional distribution $Y \\mid X$ is Gaussian.\nWe first compute the conditional mean by linear regression of $Y$ on $X$: $$ \\mathbb{E}[Y \\mid X] = aX + b, $$ where $$ a = \\frac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)}, \\qquad b = \\mathbb{E}[Y] - a,\\mathbb{E}[X]. $$ Since $\\mathrm{Cov}(X,Y) = \\rho \\sigma_x \\sigma_y$ and $\\mathrm{Var}(X) = \\sigma_x^2$, we obtain $$ \\mathbb{E}[Y \\mid X] = \\frac{\\rho \\sigma_x \\sigma_y}{\\sigma_x^2} X + \\mu_y - \\frac{\\rho \\sigma_x \\sigma_y}{\\sigma_x^2} \\mu_x = \\mu_y + \\rho \\frac{\\sigma_y}{\\sigma_x}(X - \\mu_x). $$\nNext we use the law of total variance: $$ \\mathrm{Var}(Y) = \\mathbb{E}[\\mathrm{Var}(Y \\mid X)] + \\mathrm{Var}(\\mathbb{E}[Y \\mid X]). $$\nSince $\\mathbb{E}[Y \\mid X]$ is linear in $X$, the second term is $$ \\mathrm{Var}(\\mathbb{E}[Y \\mid X]) = \\rho^2 \\frac{\\sigma_y^2}{\\sigma_x^2} \\mathrm{Var}(X) = \\rho^2 \\sigma_y^2. $$\nThus $$ \\sigma_y^2 = \\mathrm{Var}(Y) = \\mathrm{Var}(Y \\mid X) + \\rho^2 \\sigma_y^2, $$ so $$ \\mathrm{Var}(Y \\mid X) = (1 - \\rho^2)\\sigma_y^2. $$\nHence $$ Y \\mid X = x \\sim \\mathcal{N}\\Big( \\mu_y + \\rho \\frac{\\sigma_y}{\\sigma_x}(x - \\mu_x), (1 - \\rho^2)\\sigma_y^2 \\Big). $$\nNow use the moment generating function of a Gaussian.\nIf $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$, then $$ \\mathbb{E}[e^{tZ}] = \\exp\\Big(\\mu t + \\tfrac12 \\sigma^2 t^2\\Big). $$\nConditionally on $X$, we therefore have $$ \\mathbb{E}[e^{tY} \\mid X] = \\exp\\Big( \\mu_{Y \\mid X} t + \\tfrac12 \\sigma_{Y \\mid X}^2 t^2 \\Big), $$ where $$ \\mu_{Y \\mid X} = \\mu_y + \\rho \\frac{\\sigma_y}{\\sigma_x}(X - \\mu_x), \\qquad \\sigma_{Y \\mid X}^2 = (1 - \\rho^2)\\sigma_y^2. $$\nSetting $t = 1$ gives $$ \\mathbb{E}[e^{Y} \\mid X] = \\exp\\Big( \\mu_y + \\rho \\frac{\\sigma_y}{\\sigma_x}(X - \\mu_x) + \\tfrac12 (1 - \\rho^2)\\sigma_y^2 \\Big). $$\nRemark.\nThe linear regression representation $\\mathbb{E}[Y \\mid X] = aX + b$ comes from the fact that $\\mathbb{E}[Y \\mid X]$ is the minimizer of $$ \\min_f ; \\mathbb{E}\\big[(Y - f(X))^2\\big], $$ and, for jointly Gaussian $(X,Y)$, the optimal predictor is linear in $X$.\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-18-conditional-exponential-gaussian/","summary":"\u003cdiv class=\"problem-box problem-question\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eQuestion\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    Suppose $X \\sim \\mathcal{N}(\\mu_x, \\sigma_x^2)$ and $Y \\sim \\mathcal{N}(\\mu_y, \\sigma_y^2)$ are jointly Gaussian with correlation $\\rho$. Compute\n$$\n\\mathbb{E}[e^Y \\mid X].\n$$\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003eThe key step is to identify the conditional distribution of $Y$ given $X$.\u003cbr\u003e\nFor jointly Gaussian $(X,Y)$, the conditional distribution $Y \\mid X$ is Gaussian.\u003c/p\u003e\n\u003cp\u003eWe first compute the conditional mean by linear regression of $Y$ on $X$:\n$$\n\\mathbb{E}[Y \\mid X] = aX + b,\n$$\nwhere\n$$\na = \\frac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)},\n\\qquad\nb = \\mathbb{E}[Y] - a,\\mathbb{E}[X].\n$$\nSince $\\mathrm{Cov}(X,Y) = \\rho \\sigma_x \\sigma_y$ and $\\mathrm{Var}(X) = \\sigma_x^2$, we obtain\n$$\n\\mathbb{E}[Y \\mid X]\n= \\frac{\\rho \\sigma_x \\sigma_y}{\\sigma_x^2} X\n+ \\mu_y\n- \\frac{\\rho \\sigma_x \\sigma_y}{\\sigma_x^2} \\mu_x\n= \\mu_y + \\rho \\frac{\\sigma_y}{\\sigma_x}(X - \\mu_x).\n$$\u003c/p\u003e","title":"Conditional Expectation of Exponential Gaussian"},{"content":"In this page, we discuss the details of linear regression. Simple linear regression is the most commonly used method to fit a linear relationship between an observed response $y$ and an independent variable $x$. Suppose we observe $n$ data points $(x_i, y_i)$. In general, regression seeks a hypothesis $f$ such that\n$$ \\begin{aligned} \u0026amp;\\text{for each } i=1,\\ldots,n,\\ \u0026amp;\\qquad y_i = f(x_i) + \\epsilon_i, \\end{aligned} $$\nwhere $\\epsilon_i$ is an irreducible random error with mean $0$. It is commonly assumed Gaussian due to the central limit theorem.\nIn simple linear regression, we assume a linear form:\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i. $$\nThus the regression function is a parametrized model $f(x; \\beta_0, \\beta_1)$.\nFor multiple predictors $x_{1i}, x_{2i}, \\ldots, x_{pi}$, the multiple linear regression model becomes\n$$ y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\cdots + \\beta_p x_{pi} + \\epsilon_i. $$\nIn vector form:\n$$ y = X\\beta + \\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I), $$\nwhere $y \\in \\mathbb{R}^n$, $X \\in \\mathbb{R}^{n \\times (p+1)}$, and $\\beta \\in \\mathbb{R}^{p+1}$.\nThe intercept $\\beta_0$ corresponds to the first column of ones:\n$$ X = \\begin{bmatrix} \\mathbf{1} \u0026amp; x_1 \u0026amp; \\cdots \u0026amp; x_p \\end{bmatrix}. $$\nSimple linear regression is a special case of this model.\nAssumptions of Linear Regression Question The true relationship between $y$ and $x$ is linear.\nRun linear regression and plot fitted values vs.\\ residuals. The residuals should be randomly scattered around $0$. A visible pattern indicates nonlinearity.\nQuestion Errors are independent: $\\epsilon_i$ independent of $\\epsilon_j$ for $i \\ne j$.\nCheck the correlation matrix of residuals or scatterplots of residual components.\nQuestion Homoscedasticity: constant variance of errors.\nPlot fitted values vs.\\ residuals. The spread should be roughly constant — the “tube shape” pattern.\nQuestion Errors are Gaussian: $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\nCheck histograms or QQ-plots of residuals.\nQuestion Errors are independent of predictors (no endogeneity).\nPlot residuals or fitted values vs.\\ columns of $X$.\nQuestion No perfect collinearity among predictors.\nCheck condition numbers or variance inflation factors (VIF).\nThese assumptions are implicitly conditioned on $X$.\nWith this, we now discuss solutions of linear regression.\nOrdinary Least Squares Regression seeks the “best” linear fit, commonly interpreted as minimizing the residual sum of squares:\n$$ \\mathrm{RSS}(\\beta) = | y - X\\beta |^2. $$\nThis yields the ordinary least squares (OLS) estimator.\nQuestion Show that the OLS solution is $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$. Solution We minimize\n$$ \\min_{\\beta} | y - X\\beta |^2. $$\nAssuming no perfect collinearity, $X^\\top X$ is positive definite. Compute the gradient:\n$$ \\nabla_\\beta |y - X\\beta|^2 = -2X^\\top(y - X\\beta). $$\nSetting to zero:\n$$ X^\\top X \\beta = X^\\top y, $$\nso the unique minimizer is\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y. $$\nThe minimal RSS is\n$$ | (I - X(X^\\top X)^{-1}X^\\top )y |^2. $$\nSolution of Simple Linear Regression In the simple linear case $X = [\\mathbf{1} \\;\\; x]$, we compute:\n$$ X^\\top X = \\begin{bmatrix} n \u0026amp; n\\bar{x} \\ n\\bar{x} \u0026amp; \\sum_{i} x_i^2 \\end{bmatrix}, $$\nand its inverse:\n$$ (X^\\top X)^{-1} = \\frac{1}{n \\sum_i x_i^2 - n^2 \\bar{x}^2} \\begin{bmatrix} \\sum_i x_i^2 \u0026amp; -n\\bar{x} \\ -n\\bar{x} \u0026amp; n \\end{bmatrix}. $$\nCompute $X^\\top y$:\n$$ X^\\top y = \\begin{bmatrix} \\sum_i y_i \\ \\sum_i x_i y_i \\end{bmatrix}. $$\nThus, we have $$ \\hat{\\beta}_1 = \\frac{n\\sum_i x_i y_i - (\\sum_i x_i)(\\sum_i y_i)}{n\\sum_i x_i^2 - (\\sum_i x_i)^2} = \\frac{s_{xy}}{s_x^2} $$\nand\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}. $$\nSimple Linear Regression: Mean Centering Mean-centering defines $x' = x - \\bar{x}$.\nCenter $x$ only: $\\hat{\\beta}_0 = \\bar{y}$. Center $y$ only: $\\hat{\\beta}_0 = -\\hat{\\beta}_1 \\bar{x}$. Center both: $\\hat{\\beta}_0 = 0$. Also,\n$$ \\mathrm{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) = 0. $$\nInfinite Data Case Linear regression solves\n$$ \\min_{a,b} ; \\mathbb{E}[(Y - (aX + b))^2]. $$\nThe solution is\n$$ \\begin{bmatrix} a \\ b \\end{bmatrix} = \\begin{bmatrix} \\dfrac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)} \\ \\mathbb{E}[Y] - a \\mathbb{E}[X] \\end{bmatrix}. $$\nThis matches the finite-sample formulas when sample moments converge to population moments.\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-17-linear-regression-primer/","summary":"\u003cp\u003eIn this page, we discuss the details of linear regression. Simple linear regression is the most commonly used method to fit a linear relationship between an observed response $y$ and an independent variable $x$. Suppose we observe $n$ data points $(x_i, y_i)$. In general, regression seeks a hypothesis $f$ such that\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{aligned}\n\u0026amp;\\text{for each } i=1,\\ldots,n,\\\n\u0026amp;\\qquad y_i = f(x_i) + \\epsilon_i,\n\\end{aligned}\n$$\u003c/p\u003e\n\u003cp\u003ewhere $\\epsilon_i$ is an irreducible random error with mean $0$. It is commonly assumed Gaussian due to the central limit theorem.\u003c/p\u003e","title":"Linear Regression Primer"},{"content":"Definition of Brownian motion A Brownian motion $B_t$ is a stochastic process with the following properties:\nStationary increments.\nIf $s\u0026lt;t$, then $B_t - B_s$ has the same distribution as $B_{t-s} - B_0$.\nIndependent increments.\nIf $s\u0026lt;t$, then $B_t - B_s$ is independent of all $B_r$ for $r \\le s$.\nContinuous paths.\nThe function $t \\mapsto B_t$ is almost surely continuous.\nNote: The definition does not assume Gaussian increments, but from these three properties it follows that\n$$ B_t \\sim \\mathcal{N}(0,t). $$\nWe now ask the natural question:\nIf a process satisfies $X_t \\sim \\mathcal{N}(0,t)$ for all $t$, must it be Brownian motion?\nThe answer is no, as we demonstrate below.\nA Gaussian process from an SDE Consider the stochastic differential equation $$ \\begin{align} dX_t \u0026amp;= -\\lambda X_t\\, dt + \\sigma(t)\\, dB_t, \\\\ X_0 \u0026amp;= 0. \\end{align} $$\nRewriting, $$ dX_t + \\lambda X_t, dt = \\sigma(t), dB_t. $$\nMultiply both sides by the integrating factor $e^{\\lambda t}$: \\begin{align} e^{\\lambda t} dX_t + \\lambda X_t e^{\\lambda t} dt \u0026amp;= \\sigma(t) e^{\\lambda t} dB_t, \\\\ d(X_t e^{\\lambda t}) \u0026amp;= \\sigma(t) e^{\\lambda t} dB_t. \\end{align}\nIntegrating from $0$ to $t$: $$ X_t e^{\\lambda t} - X_0 = \\int_0^t \\sigma(s) e^{\\lambda s} dB_s. $$\nThus $$ X_t = \\int_0^t \\sigma(s) e^{-\\lambda (t-s)}, dB_s. $$\nSince this is a stochastic integral with deterministic integrand, $X_t$ is Gaussian with\nvariance $$ \\int_0^t |\\sigma(s) e^{-\\lambda (t-s)}|^2 ds. $$\nForcing the variance to equal $t$ We ask whether we can choose $\\sigma(t)$ such that $$ \\mathrm{Var}(X_t) = t. $$\nSo impose $$ \\int_0^t \\sigma(s)^2 e^{-2\\lambda (t-s)} ds = t. $$\nMultiply both sides by $e^{2\\lambda t}$: $$ \\int_0^t \\sigma(s)^2 e^{2\\lambda s} ds = t e^{2\\lambda t}. $$\nDifferentiate with respect to $t$: $$ \\sigma(t)^2 e^{2\\lambda t} = e^{2\\lambda t} + 2\\lambda t e^{2\\lambda t}. $$\nTherefore $$ \\sigma(t) = \\sqrt{,1 + 2\\lambda t,}. $$\nThis choice ensures $$ X_t \\sim \\mathcal{N}(0,t). $$\nBut $X_t$ is not Brownian motion Even though $X_t$ has the correct marginal distribution, its increment structure is wrong:\nincrements of $X_t$ depend on the past, because of the exponential factor $e^{-\\lambda (t-s)}$ increments are not independent increments are not stationary These can be verified directly from the covariance function: $$ \\mathbb{E}[X_t X_s] = \\int_0^{\\min(s,t)} \\sigma(u)^2 e^{-\\lambda(t-u)} e^{-\\lambda(s-u)} du, $$ which is incompatible with Brownian motion.\nThus, $X_t$ is a Gaussian process with the correct variance, but not a Brownian motion.\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-16-fake-brownian-motion/","summary":"\u003ch2 id=\"definition-of-brownian-motion\"\u003eDefinition of Brownian motion\u003c/h2\u003e\n\u003cp\u003eA Brownian motion $B_t$ is a stochastic process with the following properties:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eStationary increments.\u003c/strong\u003e\u003cbr\u003e\nIf $s\u0026lt;t$, then $B_t - B_s$ has the same distribution as $B_{t-s} - B_0$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eIndependent increments.\u003c/strong\u003e\u003cbr\u003e\nIf $s\u0026lt;t$, then $B_t - B_s$ is independent of all $B_r$ for $r \\le s$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eContinuous paths.\u003c/strong\u003e\u003cbr\u003e\nThe function $t \\mapsto B_t$ is almost surely continuous.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNote: The definition does \u003cem\u003enot\u003c/em\u003e assume Gaussian increments, but from these three properties it follows that\u003cbr\u003e\n$$\nB_t \\sim \\mathcal{N}(0,t).\n$$\u003c/p\u003e","title":"Fake Brownian Motion"},{"content":"Why do we say $(dB_t)^2 \\approx dt$ Let $B_t$ be standard Brownian motion.\nIn Itô calculus we track contributions up to order $dt$.\nAlthough ordinary calculus would ignore second-order terms, in stochastic calculus the term $(dB_t)^2$ is of order $dt$, not negligible.\nTo see this:\n$$ dB_t \\approx B_{t+\\Delta t} - B_t \\sim \\mathcal{N}(0, \\Delta t). $$\nLet $Z_{\\Delta t} \\sim \\mathcal{N}(0, \\Delta t)$.\nThen $(dB_t)^2$ behaves like $Z_{\\Delta t}^2$ and\n$$ \\mathbb{E}[Z_{\\Delta t}^2] = \\Delta t. $$\nNext compute the variance:\n$$ \\mathrm{Var}(Z_{\\Delta t}^2) = \\mathbb{E}[Z_{\\Delta t}^4] - (\\Delta t)^2. $$\nThe fourth moment of a normal is\n$$ \\mathbb{E}[Z_{\\Delta t}^4] = 3 (\\Delta t)^2, $$\nso\n$$ \\mathrm{Var}(Z_{\\Delta t}^2) = 3(\\Delta t)^2 - (\\Delta t)^2 = 2(\\Delta t)^2. $$\nAs $\\Delta t \\to 0$,\nthe mean is order $\\Delta t$, the standard deviation is order $(\\Delta t)$,\nso the fluctuation shrinks faster than the mean.\nThus $(dB_t)^2$ has a deterministic first-order contribution $\\approx dt$. Given this, the discrete Itô expansion looks like:\n$$ f(B_{t+\\Delta t}) = f(B_t) + f\u0026rsquo;(B_t)\\,\\Delta B_t + \\frac{1}{2} f\u0026rsquo;\u0026rsquo;(B_t)\\, (\\Delta B_t)^2 + o((\\Delta t)^2). $$\nSince $(\\Delta B_t)^2 \\approx \\Delta t$, this becomes\n$$ f(B_{t+\\Delta t}) - f(B_t) = f\u0026rsquo;(B_t)\\,\\Delta B_t + \\frac{1}{2} f\u0026rsquo;\u0026rsquo;(B_t)\\, \\Delta t. $$\nPassing to differential notation:\n$$ df = f\u0026rsquo;(B_t)\\, dB_t + \\frac{1}{2} f\u0026rsquo;\u0026rsquo;(B_t)\\, dt. $$\nThis shows that $df$ has drift term $\\tfrac12 f\u0026rsquo;\u0026rsquo;(B_t)$ and diffusion term $f\u0026rsquo;(B_t)$.\nFinally, note that\n$$ dB_t\\,dt \\approx 0, $$\nbecause $\\Delta B_t \\sim \\mathcal{N}(0, \\Delta t)$ gives $\\Delta B_t \\cdot \\Delta t$ of order $(\\Delta t)^{3/2}$, which is smaller than order $dt$.\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-15-brownian-motion-intuition/","summary":"\u003ch2 id=\"why-do-we-say-db_t2-approx-dt\"\u003eWhy do we say $(dB_t)^2 \\approx dt$\u003c/h2\u003e\n\u003cp\u003eLet $B_t$ be standard Brownian motion.\u003cbr\u003e\nIn Itô calculus we track contributions up to order $dt$.\u003cbr\u003e\nAlthough ordinary calculus would ignore second-order terms, in stochastic calculus\nthe term $(dB_t)^2$ is \u003cstrong\u003eof order $dt$\u003c/strong\u003e, not negligible.\u003c/p\u003e\n\u003cp\u003eTo see this:\u003c/p\u003e\n\u003cp\u003e$$\ndB_t \\approx B_{t+\\Delta t} - B_t \\sim \\mathcal{N}(0, \\Delta t).\n$$\u003c/p\u003e\n\u003cp\u003eLet $Z_{\\Delta t} \\sim \\mathcal{N}(0, \\Delta t)$.\u003cbr\u003e\nThen $(dB_t)^2$ behaves like $Z_{\\Delta t}^2$ and\u003c/p\u003e","title":"Brownian Motion Intuition"},{"content":" Question Suppose $\\mathbb{E}[|X|^k]$ exists (is finite). Does $\\mathbb{E}[|X|^m]$ exist for all $0 \\le m \\le k$? Solution Yes, $\\mathbb{E}[|X|^m]$ exists for all $0 \\le m \\le k$.\nAssume $\\mathbb{E}[|X|^k] \u0026lt; \\infty$. Then $$ \\mathbb{E}[|X|^m] = \\int_{-\\infty}^{\\infty} |x|^{m} f_X(x)\\,dx = \\int_{|x|\\le 1} |x|^{m} f_X(x)\\,dx + \\int_{|x|\u0026gt;1} |x|^{m} f_X(x)\\,dx. $$\nOn the set $|x| \\le 1$, we have $|x|^m \\le 1$, so $$ \\int_{|x|\\le 1} |x|^{m} f_X(x)\\,dx \\le \\int_{|x|\\le 1} f_X(x)\\,dx \\le 1. $$\nOn the set $|x| \u0026gt; 1$ and for $0 \\le m \\le k$, we have $|x|^m \\le |x|^k$, hence $$ \\int_{|x|\u0026gt;1} |x|^{m} f_X(x)\\,dx \\le \\int_{|x|\u0026gt;1} |x|^{k} f_X(x)\\,dx \\le \\mathbb{E}[|X|^k] \u0026lt; \\infty. $$\nCombining these, $$ \\mathbb{E}[|X|^m] \\le 1 + \\mathbb{E}[|X|^k] \u0026lt; \\infty. $$\nTherefore, $\\mathbb{E}[|X|^m]$ exists for all $0 \\le m \\le k$.\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-14-upper-bounding-absolute-power-expectation/","summary":"\u003cdiv class=\"problem-box problem-question\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eQuestion\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    Suppose $\\mathbb{E}[|X|^k]$ exists (is finite). Does $\\mathbb{E}[|X|^m]$ exist for all $0 \\le m \\le k$?\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003eYes, $\\mathbb{E}[|X|^m]$ exists for all $0 \\le m \\le k$.\u003c/p\u003e\n\u003cp\u003eAssume $\\mathbb{E}[|X|^k] \u0026lt; \\infty$. Then\n$$\n\\mathbb{E}[|X|^m]\n= \\int_{-\\infty}^{\\infty} |x|^{m} f_X(x)\\,dx\n= \\int_{|x|\\le 1} |x|^{m} f_X(x)\\,dx\n+ \\int_{|x|\u0026gt;1} |x|^{m} f_X(x)\\,dx.\n$$\u003c/p\u003e\n\u003cp\u003eOn the set $|x| \\le 1$, we have $|x|^m \\le 1$, so\n$$\n\\int_{|x|\\le 1} |x|^{m} f_X(x)\\,dx\n\\le \\int_{|x|\\le 1} f_X(x)\\,dx\n\\le 1.\n$$\u003c/p\u003e","title":"Identity for expectation of power of absolute value"},{"content":" Question How can we compute $\\pi$ using Monte Carlo, and what is the standard deviation of the estimator? Solution Correct estimator.\nGenerate i.i.d. uniform random variables $X,Y \\sim \\mathrm{Uniform}(0,1)$ and check whether $$ X^2 + Y^2 \u0026lt; 1. $$\nThe probability of this event is the area of a quarter unit circle inside the unit square: $$ \\mathbb{P}(X^2 + Y^2 \u0026lt; 1) = \\frac{\\pi}{4}. $$\nLet $$ A_n = \\sum_{i=1}^n X_i, $$ where $X_i$ is the indicator of the event ${X_i^2 + Y_i^2 \u0026lt; 1}$.\nThen $$ \\mathbb{E}[X_i] = \\frac{\\pi}{4}, $$ so the estimator $$ \\hat{\\pi}_n = \\frac{4 A_n}{n} $$ is unbiased.\nSolution Standard deviation of the estimator.\nSince $X_i$ is Bernoulli, $$ \\mathrm{Var}(X_i) = \\mathbb{E}[X_i^2] - (\\mathbb{E}[X_i])^2 = \\frac{\\pi}{4} - \\frac{\\pi^2}{16}. $$\nThus, $$ \\mathrm{Var}(A_n) = n \\left( \\frac{\\pi}{4} - \\frac{\\pi^2}{16} \\right). $$\nFor the estimator, $$ \\hat{\\pi}_n = \\frac{4A_n}{n}, $$ we have $$ \\mathrm{Var}(\\hat{\\pi}_n) = \\frac{16}{n^2} \\mathrm{Var}(A_n) = \\frac{16}{n^2} \\cdot n \\left( \\frac{\\pi}{4} - \\frac{\\pi^2}{16} \\right) = \\frac{4\\pi - \\pi^2}{n}. $$\nTherefore, the standard deviation is $$ \\sqrt{\\frac{4\\pi - \\pi^2}{n}}. $$\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-13-simulating-pi/","summary":"\u003cdiv class=\"problem-box problem-question\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eQuestion\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    How can we compute $\\pi$ using Monte Carlo, and what is the standard deviation of the estimator?\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003e\u003cstrong\u003eCorrect estimator.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eGenerate i.i.d. uniform random variables $X,Y \\sim \\mathrm{Uniform}(0,1)$ and check whether\n$$\nX^2 + Y^2 \u0026lt; 1.\n$$\u003c/p\u003e\n\u003cp\u003eThe probability of this event is the area of a quarter unit circle inside the unit square:\n$$\n\\mathbb{P}(X^2 + Y^2 \u0026lt; 1) = \\frac{\\pi}{4}.\n$$\u003c/p\u003e\n\u003cp\u003eLet\n$$\nA_n = \\sum_{i=1}^n X_i,\n$$\nwhere $X_i$ is the indicator of the event ${X_i^2 + Y_i^2 \u0026lt; 1}$.\u003cbr\u003e\nThen\n$$\n\\mathbb{E}[X_i] = \\frac{\\pi}{4},\n$$\nso the estimator\n$$\n\\hat{\\pi}_n = \\frac{4 A_n}{n}\n$$\nis \u003cstrong\u003eunbiased\u003c/strong\u003e.\u003c/p\u003e","title":"Computing Pi with Monte Carlo"},{"content":" Question Let $X,Y$ be standard normal with $\\operatorname{Cov}(X,Y)=\\tfrac{1}{\\sqrt{2}}$.\nCompute $\\mathbb{P}[X\u0026gt;0 \\mid Y\u0026lt;0]$. Solution We write $$ \\mathbb{P}[X\u0026gt;0 \\mid Y\u0026lt;0] = \\frac{\\mathbb{P}(X\u0026gt;0,\\, Y\u0026lt;0)}{\\mathbb{P}(Y\u0026lt;0)} = 2,\\mathbb{P}(X\u0026gt;0,\\, Y\u0026lt;0), $$ since $\\mathbb{P}(Y\u0026lt;0)=\\tfrac12$.\nBecause $$ Y = \\frac{1}{\\sqrt{2}}\\,X + \\frac{1}{\\sqrt{2}}\\,Z, $$ with $Z$ independent of $X$, the vector $(X,Z)$ is rotationally symmetric in $\\mathbb{R}^2$.\nThe event\n$$ X\u0026gt;0,\\qquad Y\u0026lt;0 $$ corresponds to the wedge where $X\u0026gt;0$ but $X+Z\u0026lt;0$.\nGeometrically, the plane is divided into $8$ equal $45^\\circ$ sectors, and exactly one of these sectors corresponds to $(X\u0026gt;0, X+Z\u0026lt;0)$. Thus $$ \\mathbb{P}(X\u0026gt;0,\\, Y\u0026lt;0)=\\frac18. $$\nHence $$ \\mathbb{P}[X\u0026gt;0 \\mid Y\u0026lt;0] = 2 \\times \\frac18 = \\frac14. $$\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-12-probability-of-2d-conditional-gaussian/","summary":"\u003cdiv class=\"problem-box problem-question\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eQuestion\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    Let $X,Y$ be standard normal with $\\operatorname{Cov}(X,Y)=\\tfrac{1}{\\sqrt{2}}$.\u003cbr\u003e\nCompute $\\mathbb{P}[X\u0026gt;0 \\mid Y\u0026lt;0]$.\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003eWe write\n$$\n\\mathbb{P}[X\u0026gt;0 \\mid Y\u0026lt;0]\n= \\frac{\\mathbb{P}(X\u0026gt;0,\\, Y\u0026lt;0)}{\\mathbb{P}(Y\u0026lt;0)}\n= 2,\\mathbb{P}(X\u0026gt;0,\\, Y\u0026lt;0),\n$$\nsince $\\mathbb{P}(Y\u0026lt;0)=\\tfrac12$.\u003c/p\u003e\n\u003cp\u003eBecause\n$$\nY = \\frac{1}{\\sqrt{2}}\\,X + \\frac{1}{\\sqrt{2}}\\,Z,\n$$\nwith $Z$ independent of $X$, the vector $(X,Z)$ is rotationally symmetric in $\\mathbb{R}^2$.\u003c/p\u003e\n\u003cp\u003eThe event\u003cbr\u003e\n$$\nX\u0026gt;0,\\qquad Y\u0026lt;0\n$$\ncorresponds to the wedge where $X\u0026gt;0$ but $X+Z\u0026lt;0$.\u003cbr\u003e\nGeometrically, the plane is divided into $8$ equal $45^\\circ$ sectors, and exactly \u003cstrong\u003eone\u003c/strong\u003e of these sectors corresponds to $(X\u0026gt;0, X+Z\u0026lt;0)$. Thus\n$$\n\\mathbb{P}(X\u0026gt;0,\\, Y\u0026lt;0)=\\frac18.\n$$\u003c/p\u003e","title":"Conditional probability of 2D Gaussian"},{"content":" Question Let $X_t = \\int_0^t W_\\tau , d\\tau$, where $W_t$ is a standard Wiener process.\nWhat is the distribution of $X_t$? Solution Using integration by parts, $$ X_t = \\int_0^t W_s\\, ds = t W_t - \\int_0^t s\\, dW_s = \\int_0^t (t-s)\\, dW_s. $$\nFor deterministic square-integrable $f(s)$, $$ \\int_0^t f(s), dW_s \\sim \\mathcal{N}\\!\\left( 0,\\; \\int_0^t f(s)^2\\, ds \\right). $$\nHere $f(s) = t - s$, so $$ \\int_0^t (t-s)^2\\, ds = \\frac{t^3}{3}. $$\nThus, $$ X_t \\sim \\mathcal{N}\\!\\left( 0,\\; \\frac{t^3}{3} \\right). $$\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-11-brownian-integral-distribution/","summary":"\u003cdiv class=\"problem-box problem-question\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eQuestion\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    Let $X_t = \\int_0^t W_\\tau , d\\tau$, where $W_t$ is a standard Wiener process.\u003cbr\u003e\nWhat is the distribution of $X_t$?\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003eUsing integration by parts,\n$$\nX_t = \\int_0^t W_s\\, ds\n= t W_t - \\int_0^t s\\, dW_s\n= \\int_0^t (t-s)\\, dW_s.\n$$\u003c/p\u003e\n\u003cp\u003eFor deterministic square-integrable $f(s)$,\n$$\n\\int_0^t f(s), dW_s \\sim \\mathcal{N}\\!\\left( 0,\\; \\int_0^t f(s)^2\\, ds \\right).\n$$\u003c/p\u003e\n\u003cp\u003eHere $f(s) = t - s$, so\n$$\n\\int_0^t (t-s)^2\\, ds\n= \\frac{t^3}{3}.\n$$\u003c/p\u003e","title":"Distribution of Brownian integral"},{"content":" Question How many independent $\\mathrm{Uniform}(0,1)$ random variables must we generate to ensure that with probability at least $95%$ at least one of them lies in the interval $[0.7, 0.72]$? Solution Let $E_i$ denote the event $U_i \\in [0.7, 0.72]$.\nEach has probability $$ \\mathbb{P}(E_i) = 0.02. $$\nWe want $$ \\mathbb{P}!\\left( \\bigcup_{i=1}^N E_i \\right) \\ge 0.95. $$\nSince the $U_i$ are independent, $$ \\mathbb{P}!\\left( \\bigcup_{i=1}^N E_i \\right) = 1 - \\mathbb{P}!\\left( \\bigcap_{i=1}^N E_i^c \\right) = 1 - (0.98)^N. $$\nThus we need $$ 1 - 0.98^N \\ge 0.95 \\quad\\Longleftrightarrow\\quad 0.98^N \\le 0.05. $$\nTaking logarithms, $$ N \\log(0.98) \\le \\log(0.05) \\quad\\Longrightarrow\\quad N \\ge \\frac{\\log 0.05}{\\log 0.98}. $$\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-10-number-of-rvs/","summary":"\u003cdiv class=\"problem-box problem-question\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eQuestion\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    How many independent $\\mathrm{Uniform}(0,1)$ random variables must we generate to ensure that with probability at least $95%$ at least one of them lies in the interval $[0.7, 0.72]$?\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003eLet $E_i$ denote the event $U_i \\in [0.7, 0.72]$.\u003cbr\u003e\nEach has probability\n$$\n\\mathbb{P}(E_i) = 0.02.\n$$\u003c/p\u003e\n\u003cp\u003eWe want\n$$\n\\mathbb{P}!\\left( \\bigcup_{i=1}^N E_i \\right) \\ge 0.95.\n$$\u003c/p\u003e\n\u003cp\u003eSince the $U_i$ are independent,\n$$\n\\mathbb{P}!\\left( \\bigcup_{i=1}^N E_i \\right)\n= 1 - \\mathbb{P}!\\left( \\bigcap_{i=1}^N E_i^c \\right)\n= 1 - (0.98)^N.\n$$\u003c/p\u003e","title":"Lower bound of number of independent generations"},{"content":" Question Given a biased coin with probability of heads $p \\neq \\tfrac12$, how can we simulate a fair coin? Solution Naive method (Von Neumann trick).\nConsider pairs of tosses. The outcomes $TH$ and $HT$ have equal probability: $$ \\mathbb{P}[TH] = p(1-p), \\qquad \\mathbb{P}[HT] = p(1-p). $$\nBut $$ \\mathbb{P}[HH] = p^2, \\qquad \\mathbb{P}[TT] = (1-p)^2, $$ which are not equal unless $p=\\tfrac12$.\nSo we define:\n$TH \\mapsto H$ $HT \\mapsto T$ discard $HH$ and $TT$ This produces a fair coin.\nSolution Expected number of tosses.\nOne fair outcome occurs when the pair is either $TH$ or $HT$, whose total probability is $$ \\mathbb{P}[TH] + \\mathbb{P}[HT] = 2p(1-p). $$\nEach attempt uses $2$ tosses, so the expected number of tosses is $$ 2 \\times \\frac{1}{2p(1-p)} = \\frac{1}{p(1-p)}. $$\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-09-simulating-fair-coin/","summary":"\u003cdiv class=\"problem-box problem-question\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eQuestion\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    Given a biased coin with probability of heads $p \\neq \\tfrac12$, how can we simulate a fair coin?\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003e\u003cstrong\u003eNaive method (Von Neumann trick).\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eConsider pairs of tosses. The outcomes $TH$ and $HT$ have equal probability:\n$$\n\\mathbb{P}[TH] = p(1-p), \\qquad \\mathbb{P}[HT] = p(1-p).\n$$\u003c/p\u003e\n\u003cp\u003eBut\n$$\n\\mathbb{P}[HH] = p^2, \\qquad \\mathbb{P}[TT] = (1-p)^2,\n$$\nwhich are not equal unless $p=\\tfrac12$.\u003c/p\u003e\n\u003cp\u003eSo we define:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$TH \\mapsto H$\u003c/li\u003e\n\u003cli\u003e$HT \\mapsto T$\u003c/li\u003e\n\u003cli\u003ediscard $HH$ and $TT$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis produces a fair coin.\u003c/p\u003e","title":"Simulating fair coin using unfair"},{"content":" Question For a correlation matrix of $n$ random variables:\nWhat is the sum of eigenvalues? What is a lower bound for the sum of eigenvalues of the inverse of a nonsingular $n\\times n$ correlation matrix? Solution Sum of eigenvalues.\nFor any square matrix, the sum of eigenvalues equals the trace.\nA correlation matrix has all diagonal entries equal to $1$, so $$ \\sum_{i=1}^n \\lambda_i = \\operatorname{tr}(C) = n. $$\nSolution Lower bound for the eigenvalue sum of the inverse.\nLet $\\lambda_1,\\dots,\\lambda_n$ be the eigenvalues of a nonsingular correlation matrix.\nWe know: $$ \\sum_{i=1}^n \\lambda_i = n. $$\nApply Cauchy–Schwarz to the vectors\n$\\left(\\sqrt{\\lambda_1},\\dots,\\sqrt{\\lambda_n}\\right)$ and\n$\\left(\\frac{1}{\\sqrt{\\lambda_1}},\\dots,\\frac{1}{\\sqrt{\\lambda_n}}\\right)$: $$ \\left( \\sum_{i=1}^n \\sqrt{\\lambda_i}\\cdot \\frac{1}{\\sqrt{\\lambda_i}} \\right)^2 \\le \\left( \\sum_{i=1}^n \\lambda_i \\right) \\left( \\sum_{i=1}^n \\frac{1}{\\lambda_i} \\right). $$\nThus, $$ n^2 \\le n \\sum_{i=1}^n \\frac{1}{\\lambda_i}. $$\nCanceling $n$ gives: $$ \\sum_{i=1}^n \\frac{1}{\\lambda_i} \\ge n. $$\nSince the eigenvalues of $C^{-1}$ are $\\frac{1}{\\lambda_i}$, the lower bound is $$ \\sum_{i=1}^n \\lambda_i(C^{-1}) \\ge n. $$\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-08-sum-of-eigenvalues-lower-bound/","summary":"\u003cdiv class=\"problem-box problem-question\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eQuestion\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003eFor a correlation matrix of $n$ random variables:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWhat is the sum of eigenvalues?\u003c/li\u003e\n\u003cli\u003eWhat is a lower bound for the sum of eigenvalues of the inverse of a nonsingular $n\\times n$ correlation matrix?\u003c/li\u003e\n\u003c/ol\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003e\u003cstrong\u003eSum of eigenvalues.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eFor any square matrix, the sum of eigenvalues equals the trace.\u003cbr\u003e\nA correlation matrix has all diagonal entries equal to $1$, so\n$$\n\\sum_{i=1}^n \\lambda_i = \\operatorname{tr}(C) = n.\n$$\u003c/p\u003e\n\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003e\u003cstrong\u003eLower bound for the eigenvalue sum of the inverse.\u003c/strong\u003e\u003c/p\u003e","title":"Correlation matrix sum of eigenvalues lower bound"},{"content":" Question Let $X, Y$ be jointly normal with correlation $\\rho$. Compute\n$$ \\mathbb{E}[\\operatorname{sign}(X)\\operatorname{sign}(Y)]. $$ Solution Let $Z_1, Z_2$ be independent standard normal variables and define a linear transformation so that $$ X = Z_1, \\qquad Y = \\rho Z_1 + \\sqrt{1 - \\rho^2}, Z_2. $$\nThen $$ \\mathbb{E}[\\operatorname{sign}(X)\\operatorname{sign}(Y)] = \\mathbb{P}(X\u0026gt;0, Y\u0026gt;0) + \\mathbb{P}(X\u0026lt;0, Y\u0026lt;0) - \\mathbb{P}(X\u0026gt;0, Y\u0026lt;0) - \\mathbb{P}(X\u0026lt;0, Y\u0026gt;0). $$\nBy symmetry, $$ \\mathbb{P}(X\u0026gt;0, Y\u0026gt;0) = \\mathbb{P}(X\u0026lt;0, Y\u0026lt;0), $$ and $$ \\mathbb{P}(X\u0026gt;0, Y\u0026lt;0) = \\mathbb{P}(X\u0026lt;0, Y\u0026gt;0). $$\nSince the four quadrants partition the plane, $$ 2,\\mathbb{P}(X\u0026gt;0, Y\u0026gt;0) + 2,\\mathbb{P}(X\u0026gt;0, Y\u0026lt;0) = 1. $$ Therefore $$ \\mathbb{E}[\\operatorname{sign}(X)\\operatorname{sign}(Y)] = 4,\\mathbb{P}(X\u0026gt;0, Y\u0026gt;0) - 1. $$\nThus we only need $\\mathbb{P}(X\u0026gt;0, Y\u0026gt;0)$.\nFrom the representation of $Y$, $$ Y\u0026gt;0 \\quad\\Longleftrightarrow\\quad \\rho X + \\sqrt{1-\\rho^2}, Z_2 \u0026gt; 0 \\quad\\Longleftrightarrow\\quad Z_2 \u0026gt; -\\frac{\\rho}{\\sqrt{1-\\rho^2}}, X. $$\nHence $$ \\mathbb{P}(X\u0026gt;0, Y\u0026gt;0) = \\mathbb{P}!\\left( X\u0026gt;0,; Z_2 \u0026gt; -\\frac{\\rho}{\\sqrt{1-\\rho^2}}, X \\right). $$\nLet $$ \\alpha = -\\frac{\\rho}{\\sqrt{1-\\rho^2}}. $$\nWe need $$ \\mathbb{P}(x\u0026gt;0,\\ z\u0026gt;\\alpha x) = \\iint_{x\u0026gt;0,\\ z\u0026gt;\\alpha x} \\frac{1}{2\\pi} \\exp!\\left(-\\frac12(x^2 + z^2)\\right), dz, dx. $$\nSwitch to polar coordinates $x = r\\cos\\theta$, $z = r\\sin\\theta$.\nThe line $z = \\alpha x$ becomes $\\tan\\theta = \\alpha$, i.e. $\\theta = \\arctan(\\alpha)$.\nThus the region $x\u0026gt;0$ and $z\u0026gt;\\alpha x$ corresponds to $$ \\theta \\in [\\arctan(\\alpha),, \\tfrac{\\pi}{2}], \\qquad r\u0026gt;0. $$\nCompute: $$ \\mathbb{P}(X\u0026gt;0, Y\u0026gt;0) = \\frac{1}{2\\pi} \\int_{\\arctan(\\alpha)}^{\\pi/2} \\int_0^\\infty r, e^{-r^2/2}, dr, d\\theta. $$\nThe $r$–integral is $$ \\int_0^\\infty r, e^{-r^2/2}, dr = 1. $$\nHence $$ \\mathbb{P}(X\u0026gt;0, Y\u0026gt;0) = \\frac{1}{2\\pi}\\left( \\frac{\\pi}{2} - \\arctan(\\alpha) \\right). $$\nRecall $\\alpha = -\\frac{\\rho}{\\sqrt{1-\\rho^2}}$, so $$ \\arctan(\\alpha) = -\\arctan!\\left(\\frac{\\rho}{\\sqrt{1-\\rho^2}}\\right) = -\\arcsin(\\rho). $$\nThus $$ \\mathbb{P}(X\u0026gt;0, Y\u0026gt;0) = \\frac{1}{2\\pi}\\left( \\frac{\\pi}{2} + \\arcsin(\\rho) \\right). $$\nFinally, $$ \\mathbb{E}[\\operatorname{sign}(X)\\operatorname{sign}(Y)] = 4,\\mathbb{P}(X\u0026gt;0, Y\u0026gt;0) - 1 = \\frac{2}{\\pi}\\arcsin(\\rho). $$\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-06-product-of-sign-expectation/","summary":"\u003cdiv class=\"problem-box problem-question\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eQuestion\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    Let $X, Y$ be jointly normal with correlation $\\rho$.  Compute\u003cbr\u003e\n$$\n\\mathbb{E}[\\operatorname{sign}(X)\\operatorname{sign}(Y)].\n$$\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003eLet $Z_1, Z_2$ be independent standard normal variables and define a linear transformation so that\n$$\nX = Z_1, \\qquad\nY = \\rho Z_1 + \\sqrt{1 - \\rho^2}, Z_2.\n$$\u003c/p\u003e\n\u003cp\u003eThen\n$$\n\\mathbb{E}[\\operatorname{sign}(X)\\operatorname{sign}(Y)]\n= \\mathbb{P}(X\u0026gt;0, Y\u0026gt;0) + \\mathbb{P}(X\u0026lt;0, Y\u0026lt;0)\n- \\mathbb{P}(X\u0026gt;0, Y\u0026lt;0) - \\mathbb{P}(X\u0026lt;0, Y\u0026gt;0).\n$$\u003c/p\u003e\n\u003cp\u003eBy symmetry,\n$$\n\\mathbb{P}(X\u0026gt;0, Y\u0026gt;0) = \\mathbb{P}(X\u0026lt;0, Y\u0026lt;0),\n$$\nand\n$$\n\\mathbb{P}(X\u0026gt;0, Y\u0026lt;0) = \\mathbb{P}(X\u0026lt;0, Y\u0026gt;0).\n$$\u003c/p\u003e","title":"Expectation of $\\text{sgn}(X)\\cdot\\text{sgn}(Y)$"},{"content":" Question Let $W_t = (X_t, Y_t)$ be planar Brownian motion started at $(x,y)$ with $x\u0026gt;0$, $y\u0026gt;0$.\nWhat is the probability that the process hits the $y$-axis before the $x$-axis, i.e.\n$\\mathbb{P}[\\tau_y \u0026lt; \\tau_x]$, where $$ \\tau_x = \\min\\{t : Y_t = 0\\}, \\qquad \\tau_y = \\min\\{t : X_t = 0\\}? $$ Solution Define $\\tau = \\min\\{\\tau_x, \\tau_y\\}$ and let $u(x,y)$ be twice differentiable.\nItô\u0026rsquo;s formula gives\n$$ u(X_\\tau, Y_\\tau) - u(x,y) = \\int_0^\\tau u_x dX_t + \\int_0^\\tau u_y dY_t + \\frac12 \\int_0^\\tau (u_{xx} + u_{yy}) dt. $$\nChoose $u$ to solve the boundary value problem $$ \\begin{cases} \\Delta u = 0, \u0026amp; x\u0026gt;0,\\ y\u0026gt;0 \\\\ u(0,y) = 1, \u0026amp; y\u0026gt;0 \\\\ u(x,0) = 0, \u0026amp; x\u0026gt;0. \\end{cases} $$\nSince $\\Delta u = 0$ and the stochastic integrals have mean zero, $$ u(x,y) = \\mathbb{E}[u(X_\\tau, Y_\\tau)]. $$ On the boundary, $u=1$ on the $y$-axis and $u=0$ on the $x$-axis, so $$ u(x,y) = 1 \\cdot \\mathbb{P}[\\tau_y \u0026lt; \\tau_x] + 0 \\cdot \\mathbb{P}[\\tau_x \u0026lt; \\tau_y] = \\mathbb{P}[\\tau_y \u0026lt; \\tau_x]. $$\nThus we compute $u$ from Laplace\u0026rsquo;s equation with the stated boundary conditions.\nSwitch to polar coordinates $(r,\\theta)$ with $$ x = r\\cos\\theta,\\qquad y = r\\sin\\theta,\\qquad 0 \u0026lt; \\theta \u0026lt; \\frac{\\pi}{2}. $$ The angular boundary conditions become $$ u(r,0) = 0,\\qquad u\\left(r, \\frac{\\pi}{2} \\right) = 1. $$\nLook for solutions independent of $r$: $u(r,\\theta) = f(\\theta)$.\nThen $\\Delta u = 0$ reduces to $$ f\u0026rsquo;\u0026rsquo;(\\theta) = 0, $$ so $f(\\theta) = a + c\\theta$.\nUse the boundary values: $$ f(0) = 0 \\implies a = 0, \\ f\\left(\\frac{\\pi}{2}\\right) = 1 \\implies c\\frac{\\pi}{2} = 1 \\implies c = \\frac{2}{\\pi}. $$\nTherefore $$ u(r,\\theta) = \\frac{2}{\\pi}\\theta. $$\nSince $\\theta = \\arctan(y/x)$ for the starting point $(x,y)$, we obtain $$ \\mathbb{P}[\\tau_y \u0026lt; \\tau_x] = u(x,y) = \\frac{2}{\\pi} \\arctan\\left(\\frac{y}{x}\\right). $$\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-05-2d-wiener-process-stopping/","summary":"\u003cdiv class=\"problem-box problem-question\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eQuestion\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    Let $W_t = (X_t, Y_t)$ be planar Brownian motion started at $(x,y)$ with $x\u0026gt;0$, $y\u0026gt;0$.\u003cbr\u003e\nWhat is the probability that the process hits the $y$-axis before the $x$-axis, i.e.\u003cbr\u003e\n$\\mathbb{P}[\\tau_y \u0026lt; \\tau_x]$, where\n$$\n\\tau_x = \\min\\{t : Y_t = 0\\}, \\qquad\n\\tau_y = \\min\\{t : X_t = 0\\}?\n$$\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003eDefine $\\tau = \\min\\{\\tau_x, \\tau_y\\}$ and let $u(x,y)$ be twice differentiable.\u003c/p\u003e\n\u003cp\u003eItô\u0026rsquo;s formula gives\u003c/p\u003e\n\u003cp\u003e$$\nu(X_\\tau, Y_\\tau) - u(x,y)\n= \\int_0^\\tau u_x dX_t\n+ \\int_0^\\tau u_y dY_t\n+ \\frac12 \\int_0^\\tau (u_{xx} + u_{yy}) dt.\n$$\u003c/p\u003e","title":"Stopping time for 2D Wiener process"},{"content":" Question Give the upper and lower bounds on the correlation parameter (\\rho) for the matrix $$ C=\\begin{pmatrix} 1 \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; \\rho \\\\ \\rho \u0026amp; 1 \u0026amp; \\cdots \u0026amp; \\rho \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\rho \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; 1 \\end{pmatrix}. $$ Solution We write\n$$ C = (1-\\rho) I + \\rho M, $$ where $M$ is the $n\\times n$ matrix of all ones.\nEigenvalues of $M$ The matrix $M$ satisfies:\nOne eigenvalue $n$ with eigenvector $(1,1,\\dots,1)^{\\top}$. This is because any nonzero eigenvector must satisfy $Mv = \\lambda v$, and $\\sum_iv_i = \\lambda v_i$ for all $1\\le i\\le n$; $\\lambda v_1 = \\cdots = \\lambda v_n$, therefore $nv_1 = \\lambda v_1 \\Leftrightarrow \\lambda = n$.\nOne eigenvalue $0$ with multiplicity $n-1$.\nThus the eigenvalues of $C = (1-\\rho)I + \\rho M$ are:\n$1-\\rho + \\rho n = 1 + (n-1)\\rho$, multiplicity $1$, $1-\\rho$, multiplicity $n-1$. Positive semidefiniteness constraints Since $C$ is a correlation matrix, all eigenvalues must be nonnegative:\n$1-\\rho \\ge 0 \\quad \\Longrightarrow\\quad \\rho \\le 1.$\n$1 + (n-1)\\rho \\ge 0 \\quad \\Longrightarrow\\quad \\rho \\ge -\\frac{1}{,n-1,}.$\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-04-equicorrelation-matrix/","summary":"\u003cdiv class=\"problem-box problem-question\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eQuestion\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    Give the upper and lower bounds on the correlation parameter (\\rho) for the matrix\n$$\nC=\\begin{pmatrix}\n1 \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; \\rho \\\\\n\\rho \u0026amp; 1 \u0026amp; \\cdots \u0026amp; \\rho \\\\\n\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\n\\\\\n\\rho \u0026amp; \\rho \u0026amp; \\cdots \u0026amp; 1\n\\end{pmatrix}.\n$$\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003eWe write\u003cbr\u003e\n$$\nC = (1-\\rho) I + \\rho M,\n$$\nwhere $M$ is the $n\\times n$ matrix of all ones.\u003c/p\u003e","title":"Correlation matrix with constant correlation"},{"content":" Question Let $W_t$ be the standard Wiener process, what is $\\mathbb{E}[e^{W_t}]$? Solution We will use the moment generating function (MGF) for $W_t\\sim\\mathcal{N}(0,t)$. The MGF for Gaussian is $M(s) = \\exp\\left(\\mu s+ (1/2)\\sigma^2s^2\\right)$, plug in $\\mu=0, \\sigma=\\sqrt{t}$, we have $M(t) = e^{(1/2)s^2}$ and $M(1) = \\mathbb{E}[e^{W_t}] = \\exp((1/2)t)$. ","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-03-expectation-of-exponential-wiener/","summary":"\u003cdiv class=\"problem-box problem-question\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eQuestion\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    Let $W_t$ be the standard Wiener process, what is $\\mathbb{E}[e^{W_t}]$?\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    We will use the moment generating function (MGF) for $W_t\\sim\\mathcal{N}(0,t)$. The MGF for Gaussian is $M(s) = \\exp\\left(\\mu s+ (1/2)\\sigma^2s^2\\right)$, plug in $\\mu=0, \\sigma=\\sqrt{t}$, we have $M(t) = e^{(1/2)s^2}$ and $M(1) = \\mathbb{E}[e^{W_t}] = \\exp((1/2)t)$.\n  \u003c/div\u003e\n\u003c/div\u003e","title":"Expected value of exponential Wiener process"},{"content":" Question Let $X\\sim \\mathcal{N}(\\mu,\\sigma^2)$, $\\Phi$ denotes the standard Gaussian CDF. What is $\\mathbb{E}[\\Phi(X)]$? Solution By definition $\\Phi(X) = P[Z \\le X]$ where $Z$ is a standard Gaussian variable. Then\n$$ \\Phi(X) = P[Z \\le X] = P[Z - X \\le 0] $$\nwhere $Z - X \\sim \\mathcal{N}(-\\mu, 1+\\sigma^2)$. Therefore\n$$ \\begin{aligned} P[Z - X \\le 0] \u0026amp;= P\\!\\left( \\frac{Z - X + \\mu}{\\sqrt{1+\\sigma^2}} \\le \\frac{\\mu}{\\sqrt{1+\\sigma^2}} \\right) \\\\ \u0026amp;= P\\!\\left( \\tilde{Z} \\le \\frac{\\mu}{\\sqrt{1+\\sigma^2}} \\right) \\\\ \u0026amp;= \\Phi\\!\\left( \\frac{\\mu}{\\sqrt{1+\\sigma^2}} \\right). \\end{aligned} $$\nA special case is when $X$ has zero mean, in which case $\\mathbb{E}[\\Phi(X)] = 1/2$.\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-02-expectation-of-cdf/","summary":"\u003cdiv class=\"problem-box problem-question\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eQuestion\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    Let $X\\sim \\mathcal{N}(\\mu,\\sigma^2)$, $\\Phi$ denotes the standard Gaussian CDF. What is $\\mathbb{E}[\\Phi(X)]$?\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003eBy definition $\\Phi(X) = P[Z \\le X]$ where $Z$ is a standard Gaussian variable. Then\u003c/p\u003e\n\u003cp\u003e$$\n\\Phi(X) = P[Z \\le X] = P[Z - X \\le 0]\n$$\u003c/p\u003e\n\u003cp\u003ewhere $Z - X \\sim \\mathcal{N}(-\\mu, 1+\\sigma^2)$. Therefore\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{aligned}\nP[Z - X \\le 0]\n\u0026amp;= P\\!\\left(\n\\frac{Z - X + \\mu}{\\sqrt{1+\\sigma^2}}\n\\le \\frac{\\mu}{\\sqrt{1+\\sigma^2}}\n\\right) \\\\\n\u0026amp;= P\\!\\left(\n\\tilde{Z} \\le \\frac{\\mu}{\\sqrt{1+\\sigma^2}}\n\\right) \\\\\n\u0026amp;= \\Phi\\!\\left(\n\\frac{\\mu}{\\sqrt{1+\\sigma^2}}\n\\right).\n\\end{aligned}\n$$\u003c/p\u003e","title":"Expectation of Gaussian CDF"},{"content":" Question Consider the regression $Y\\sim X$ which gives us coefficient $\\beta$, now if we do $X\\sim Y$, what is the range of the new coefficient? Solution We have $$ \\beta = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)} = \\rho\\frac{\\sigma_Y}{\\sigma_X} $$ where $\\rho$ is the correlation; $\\sigma_X,\\sigma_Y$ are the standard deviations.\nThis means $$ \\beta_{\\text{new}} = \\frac{\\text{Cov(X,Y)}}{\\text{Var}(Y)} = \\rho\\cdot\\frac{\\sigma_X}{\\sigma_Y} = \\rho^2\\cdot\\frac{1}{\\beta} $$\nAnd because $0\\le\\rho^2\\le 1$, we have $$ \\beta_{\\text{new}} \\in [0, 1/\\beta]. $$\n","permalink":"https://honglizhaobob.github.io/math-problems/problems/2025-10-10-01-range-of-regression-coeff/","summary":"\u003cdiv class=\"problem-box problem-question\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eQuestion\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    Consider the regression $Y\\sim X$ which gives us coefficient $\\beta$, now if we do $X\\sim Y$, what is the range of the new coefficient?\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\n\u003cdiv class=\"problem-box problem-solution\"\u003e\n  \u003cdiv class=\"problem-box-label\"\u003eSolution\u003c/div\u003e\n  \u003cdiv class=\"problem-box-body\"\u003e\n    \u003cp\u003eWe have\n$$\n\\beta = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)} = \\rho\\frac{\\sigma_Y}{\\sigma_X}\n$$ where $\\rho$ is the correlation; $\\sigma_X,\\sigma_Y$ are the standard deviations.\u003c/p\u003e\n\u003cp\u003eThis means\n$$\n\\beta_{\\text{new}} = \\frac{\\text{Cov(X,Y)}}{\\text{Var}(Y)} = \\rho\\cdot\\frac{\\sigma_X}{\\sigma_Y} = \\rho^2\\cdot\\frac{1}{\\beta}\n$$\u003c/p\u003e\n\u003cp\u003eAnd because $0\\le\\rho^2\\le 1$, we have\n$$\n\\beta_{\\text{new}} \\in [0, 1/\\beta].\n$$\u003c/p\u003e\n\n  \u003c/div\u003e\n\u003c/div\u003e","title":"Range of regression coefficient"}]